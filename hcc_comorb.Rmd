---
title: "Simple examples of Bayesian modeling of Medicare cost"
author: "Mike Arciero"
date: "3/26/2022"
output: 
    html_document:
    toc: true
    toc_depth: 2
---

```{r,echo = FALSE, include = FALSE}
knitr::opts_chunk$set(
  echo = FALSE, 
  warning = FALSE, 
  message = FALSE,
  dev = "png",
  dpi = 150,
  fig.align = "center",
  comment = NA
)


library(tidyverse)
library(tidymodels)
library(rstan)
library(rstanarm)
library(bayesplot)
library(bayesrules)
library(broom)
library(broom.mixed)
# library(rethinking)
library(janitor)
library(comorbidity)


theme_set(bayesplot::theme_default())

# seed for R's pseudo-RNGs, not Stan's
set.seed(1123) 

```


``` {r, echo = F, include = FALSE}
augusta_raw <- read_csv("data/augusta_2020.csv")

## get rid of spaces and also remove decimals in ICD codes, as the lookup table has none.
augusta <- augusta_raw %>% rename_with(~ str_replace_all(., " ", "_"), "PATIENT NBR": "CPT 15") %>%
    mutate(across(DIAG_1:DIAG_15, ~str_remove(., "\\.")))

nrow(augusta)
n_distinct(augusta$PATIENT_NBR)

augusta %>% group_by(PATIENT_NBR) %>% summarize(count = n())  %>%
    ggplot(aes(count)) +
    geom_histogram(aes(y = ..density..), binwidth = 1, color = "white")

### create column with visit number
visit_df <- augusta %>% group_by(PATIENT_NBR) %>% summarize(count = n()) %>%
    mutate(visit = map(count, function(i){c(1:i)})) %>%
    unnest(visit)

augusta_vis <- cbind(visit_df[, 3], augusta)

## The HCC lookup table
hcc <- read_csv("data/HCC_mappings_2020.csv")
colnames(hcc)[3] <- "HCC"
hcc <- hcc %>% filter(!is.na(HCC))
hcc$HCC <- as.factor(hcc$HCC)
hcc$Code <- as.factor(hcc$Code)

augusta_long <- augusta_vis %>% pivot_longer(DIAG_1:DIAG_15, names_to = "diagnosis_no", values_to = "code") %>%
         select(PATIENT_NBR, visit, FIN_CLASS, DRG, diagnosis_no, code) %>% select(-DRG) %>%
    filter(!is.na(code))
augusta_long$code <- as.factor(augusta_long$code)

## some codes mapped to more that one HCC.
augusta_long_hcc <- augusta_long %>% left_join(hcc, by = c("code" = "Code")) # %>%
    ##  filter(!is.na(HCC))  ## previously we dropped records with no HCC.

 n_distinct(augusta_long_hcc$HCC)

 n_distinct(augusta_long_hcc$HCC)

 ### Pick out the distinct HCC for each patient, number of distinct HCCs, number of visits, generate age based on
 ### number of hcc categories, generate financial/income, gender.

 N= nrow(augusta_long_hcc %>% group_nest(PATIENT_NBR))
 set.seed(123)
 augusta_unique_hcc <- augusta_long_hcc %>% group_nest(PATIENT_NBR) %>%
     mutate(HCC_unique = map(data, ~unique(.x$HCC))) %>%
     mutate(visits = map_int(data, ~max(.x$visit))) %>%
   mutate(hcc_conds = map_int(HCC_unique, ~length(.) - sum(is.na(.x)))) %>%  ## subtracts NA, which were counted distinct HCC
     mutate(age = 65 + 2*hcc_conds + rnorm(N, 0 , sd = 5))  %>%   ## fake data
     mutate(across(age, ~floor(.x))) %>%
     mutate(gender = as.factor(sample(c("M", "F", "NB"), N, prob = c(0.49, 0.49, 0.02),  replace = TRUE)))%>%  ## more fake data
     mutate(financial = ordered(sample(c(1:3), N, replace = TRUE)))

```


## Overview

We add Charlson and Elixhauser comorbidity scores to our sample hospital data for use as predictors. These scores are weighted sums based on the presence of chronic conditions - seventeen for Charlson and thirty one for Elixhauser. We then add fake normally distributed cost data with mean a linear function of the predictors

$$
 \textrm{cost}_i \sim \cal{N}(\mu_i, 10) \\
 \mu_i = 15 + \textrm{visits}_i + 2 \times \textrm{hcc_conds}_i +
                                       3 \times \textrm{comorb_score}_i + 2 \times \textrm{age_norm}_i
$$
where `cost` is in \$ $\times 1000$  and `age_norm` is standardized age. Thus `cost` increases \$2000 dollars for an increase of of one standard deviation in age. Of course this is something we can change. The standard deviation of 10 was chosen arbitrarily. We will initially use Charlson comorbidity score for `comorb_score` but we can use either or both. We will then fit a Bayesian model and see how well we recover the parameters.

## Mapping ICD 10 codes to comorbidity scores

We use an R package to do this.

```{r, echo = FALSE, include = FALSE}
df <- unnest(augusta_unique_hcc, data)

    elix <- comorbidity(df, id = "PATIENT_NBR", code = "code", map = "elixhauser_icd10_quan", assign0 = FALSE)
    char <- comorbidity(df, id = "PATIENT_NBR", code = "code", map = "charlson_icd10_quan", assign0 = FALSE)
comorbidity(df, id = "PATIENT_NBR", code = "code", map = "charlson_icd10_quan", assign0 = FALSE)

morb_idx <- cbind(elix, elix_score = score(elix, weights = "swiss", assign0 = T)) %>% full_join(
cbind(char, char_score = score(char, assign0 = T)), by = "PATIENT_NBR") %>%
    select(PATIENT_NBR, char_score, elix_score)

df <- augusta_unique_hcc %>% full_join(morb_idx, by = "PATIENT_NBR")

```


Here is what the scores look like

```{r, include = TRUE}
 head(char,5)
head(elix, 5)
```
The scores are weighted sums of these

```{r}
df %>% select(PATIENT_NBR, char_score, elix_score) %>% head()
```
We can check correllations with `visits`, for example

```{r, echo=T}
cor(df$visits, df$char_score)
cor(df$visits, df$elix_score)

```

## Add fake cost data

Note the `age` are normalized values.

```{r, echo = FALSE, include=T}
cost_df <- df %>% mutate(cost = rnorm(nrow(df), 15 + visits + 2*hcc_conds +
                                       3*char_score + 2*(age - 71.6)/7.3 , 10))

cost_df_spl <- slice_sample(cost_df, n = 200, replace = FALSE)

rec <- recipe(cost ~  visits + hcc_conds + char_score + age, data = cost_df_spl) %>%
    step_normalize(age)

### weird to bake  a subset but whatever
cost_prep <- rec %>% prep(cost_df) %>% bake(cost_df_spl)

```

``` {r, echo = FALSE, include = TRUE}
head(cost_prep)
```

## Fit the model to the fake data

To for ease of computation we fit the model to a sample of size 200 rather than the entire 4000 plus data record.

```{r,  include = FALSE}
cost_fake = list(cost = cost_prep$cost, N = nrow(cost_prep), visits = cost_prep$visits,
                 hcc_count = cost_prep$hcc_conds, char_score = cost_prep$char_score,
                 age_norm = cost_prep$age)

## Use same stan file
cost_cp <- stan_model("cost_01.stan")

cost_sim <- sampling(cost_cp, data = cost_fake, chains = 4, iter = 1000)

```

## Posterior checks

We look at the summary stats for the parameters and their distributions. The model appears to do a reasonable job of reproducing the parameters.

```{r, include=TRUE}
library(rethinking)
cost_sim_df <- as.data.frame(cost_sim)
precis(cost_sim_df %>%  select(c(alpha, beta_vs, beta_hcc, beta_char, beta_age)))
```

```{r, include=TRUE}
mcmc_dens(cost_sim, pars = c("alpha", "beta_vs", "beta_hcc", "beta_char", "beta_age"))
```


To see how well the model reproduces the data we can compare the distributions for the cost predicted by the model for each of a sample of 100 patients, $y_{rep}$, with the actual distribution of cost for all of the patients, $y$. 

```{r, include=TRUE}
y_rep <- as.matrix(cost_sim, pars = "y_rep")
ppc_dens_overlay(y = cost_df_spl$cost, yrep = y_rep[1:100, ])
```

We also check visually check how well the distributions for the parameter compare with the true values.

```{r, include = TRUE}
posterior_params <- as.data.frame(cost_sim, pars = c("alpha", "beta_vs", "beta_hcc", "beta_char", "beta_age"))
true_params <- c(15, 1, 2, 3, 2)
mcmc_recover_hist(posterior_params, true_params)
```

## A further check      

Rather than choosing model coefficients and fitting to the data, one way to check both the model and the data generating process is to simulate both the coefficients and the predictor values from appropriate distributions, then generate fake data, then fit the model and check against the real data. (In our case the "real" data includes fake age and cost data.)

We generate a single sample of the coefficients

$$
 \alpha \sim  \cal{N}(15, 4) \\
 \beta_{vs} \sim  \cal{N}(1, 1) \\
  \beta_{hcc} \sim  \cal{N}(2, 2) \\
   \beta_{char} \sim  \cal{N}(3, 1) \\
    \beta_{age} \sim  \cal{N}(2, 1) \\
    \sigma \sim  \textrm{exponential}(1) 
 $$
We then generate 200 samples of predictors and `cost`.

$$ 
 \textrm{visits}_i \sim \textrm{Poisson}(2.5) \\
 \textrm{hcc_count}_i \sim \textrm{NB}(3.5, 3.2) \\
 \textrm{char_score}_i \sim \textrm{Binomial}(8, 0.25) \\
 \textrm{age_norm} \sim \cal{N}(0, 1) \\
 \textrm{cost} \sim \cal{N}(\mu_i, \sigma) \\
\mu_i = \alpha + \beta_{vs} \times \textrm{visits}_i + \beta_{hcc}\times \textrm{hcc_conds}_i +
                                      \beta_{char} \times  \textrm{comorb_score}_i + \beta_{age} \times \textrm{age_norm}_i
$$

Then we fit our model to this simultated data

```{r, echo = FALSE, include = FALSE}
priors_cp <- stan_model("cost_01_priors.stan")
priors_sim <- sampling(priors_cp, chains = 1, iter = 1, algorithm = 'Fixed_param')

samps <- rstan::extract(priors_sim)

### fit the model to this fake generated data as in Gabry Helsinki

cost_fake_fake <- list(N = 200, visits = samps$visits[1, ],
                            hcc_count = samps$hcc_count[1, ], char_score = samps$char_score[1, ],
                            age_norm = samps$age[1, ], cost = samps$cost[1, ])

samps$cost[1,] %>% class()

cost_sim_fake <- sampling(cost_cp, data = cost_fake_fake, chains = 4, iter = 1000)
```
We can again see how well the parameters were recovered

```{r, echo = FALSE, include = TRUE}
cost_sim_fake_df <- as.data.frame(cost_sim_fake)
precis(cost_sim_fake_df %>%  select(c(alpha, beta_vs, beta_hcc, beta_char, beta_age)))

mcmc_dens(cost_sim_fake, pars = c("alpha", "beta_vs", "beta_hcc", "beta_char", "beta_age"))

```
```{r, echo = TRUE, include = TRUE}
samps$alpha
samps$beta_vs
samps$beta_hcc
samps$beta_char
samps$beta_age
```

The parameter estimates appear close, but the credible intervals do not all contain the actual values. This may suggest something about the actual generating process...
```{r, echo = FALSE, include = TRUE}
posterior_params <- as.data.frame(cost_sim_fake, pars = c("alpha", "beta_vs", "beta_hcc", "beta_char", "beta_age"))
true_params <- c(samps$alpha, samps$beta_vs, samps$beta_hcc, samps$beta_char, samps$beta_age)
mcmc_recover_hist(posterior_params, true_params)
```



We can also check the density overlays for the cost as before.

```{r, echo = FALSE, include = TRUE}
y_rep <- as.matrix(cost_sim_fake, pars = "y_rep")
ppc_dens_overlay(y = cost_fake_fake$cost, yrep = y_rep[1:200, ])
```

## Stratifications

Leaving questions of generating process aside for the moment, we remark that from here it is an easy exercise to find distributions for patients with a given set of predictor values, or to compute likelihood ratios for patients from different strata, as in the Ibis example.

